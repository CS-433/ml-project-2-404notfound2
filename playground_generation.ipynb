{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Causal Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lyh\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COPA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (C:/Users/lyh/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
      "100%|██████████| 3/3 [00:00<00:00, 110.39it/s]\n"
     ]
    }
   ],
   "source": [
    "copa = load_dataset(\"super_glue\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'I drank from the water fountain.',\n",
       " 'choice1': 'I was thirsty.',\n",
       " 'choice2': 'I felt nauseous.',\n",
       " 'question': 'cause',\n",
       " 'idx': 50,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See one example\n",
    "\n",
    "copa[\"train\"][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "print(len(copa[\"train\"]))\n",
    "print(len(copa[\"validation\"]))\n",
    "print(len(copa[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "See [Multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice).\n",
    "\n",
    "Here, we use `AutoModelForMultipleChoice` for the baseline. The model receives one input sentence as question and several sentences as candidates. Then the model predicts the correct answer sentence by text classification. Here we use `premise` with `question` as query and choice_i as candidates.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "- `query`: my body cast a shadow over the grass because\n",
    "- `candidates1`: the sun was rising.\n",
    "- `candidates2`: the grass was cut.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n",
    "\n",
    "- `query`: the elderly woman suffered a stroke so\n",
    "- `candidates1`: the woman's daughter came over to clean her house.\n",
    "- `candidates2`: the woman's daughter moved in to take care of her."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "See [Multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice).\n",
    "\n",
    "Here, we use `AutoModelForMultipleChoice` for the baseline. The model receives one input sentence as question and several sentences as candidates. Then the model predicts the correct answer sentence by text classification. Here we use `premise` with `question` as query and choice_i as candidates.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "- `query`: my body cast a shadow over the grass because\n",
    "- `candidates1`: the sun was rising.\n",
    "- `candidates2`: the grass was cut.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n",
    "\n",
    "- `query`: the elderly woman suffered a stroke so\n",
    "- `candidates1`: the woman's daughter came over to clean her house.\n",
    "- `candidates2`: the woman's daughter moved in to take care of her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt_create = generate_prompt()\\n\\nresponse = openai.Completion.create(\\n  model=\"text-davinci-003\",\\n  prompt=prompt_create,\\n  temperature=0.7,\\n  max_tokens=256,\\n  top_p=1,\\n  frequency_penalty=0,\\n  presence_penalty=0\\n)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "'''\n",
    "prompt_create = generate_prompt()\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=prompt_create,\n",
    "  temperature=0.7,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_direct(example):\n",
    "    return \"\"\" Get Answer from Premise and Question.\n",
    "    Premise:The teacher took roll.\n",
    "    Question:What is the effect of it?\n",
    "    Answer:She identified the students that were absent.\n",
    "    Premise:The book was deemed inappropriate for children.\n",
    "    Question:What is the effect of it?\n",
    "    Answer:Schools banned it from its libraries.\n",
    "    Premise:The child caught a life-threatening illness.\n",
    "    Question:What is the cause of it?\n",
    "    Answer:She didn't get vaccinated.\n",
    "    Premise:The young woman was denied entrance into the bar.\n",
    "    Question:What is the cause of it?\n",
    "    Answer:She forgot her ID.\n",
    "    Premise:The man had lipstick on his cheek.\n",
    "    Question:What is the cause of it?\n",
    "    Answer:The woman kissed him.\n",
    "    Premise:The woman had a religious awakening.\n",
    "    Question:What is the effect of it?\n",
    "    Answer:She began going to church.\n",
    "    Premise:The woman deleted the email.\n",
    "    Question:What is the cause of it?\n",
    "    Answer:The sender tried to solicit money from her.\n",
    "    Premise:The girl wanted to make her mother happy.\n",
    "    Question:What is the effect of it?\n",
    "    Answer:The girl picked a flower for her mother.\n",
    "    Premise:The girl applied the scissors to the paper.\n",
    "    Question:What is the effect of it?\n",
    "    Answer:The paper sliced apart.\n",
    "    Premise:I got distracted from my conversation with the woman.\n",
    "    Question:What is the cause of it?\n",
    "    Answer:Everyone in the room was talking.\n",
    "    Premise: {}\n",
    "    Question: What is the {} of it?\n",
    "    Answer:\n",
    "    \n",
    "    \"\"\".format(\n",
    "        example['premise'],\n",
    "        example['question'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'I got distracted from my conversation with the woman.',\n",
       " 'choice1': 'Everyone in the room was talking.',\n",
       " 'choice2': 'The woman was telling a funny story.',\n",
       " 'question': 'cause',\n",
       " 'idx': 240,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa[\"train\"][240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(example):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=generate_prompt_direct(example),\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# load word2vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('path to word2vec e.g. GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# your inputs\n",
    "first_sentence_list = ['driver', 'backs', 'into', 'stroller', 'with', 'child', ',', 'drives', 'off']\n",
    "second_sentence_list = ['driver', 'backs', 'into', 'mom', ',', 'stroller', 'with', 'child', 'then', 'drives', 'off']\n",
    "\n",
    "# remove oov\n",
    "first = [word for word in first_sentence_list if word in model.key_to_index]\n",
    "second = [word for word in second_sentence_list if word in model.key_to_index]\n",
    "\n",
    "# average word embeddings to get sentence embeddings\n",
    "first_sent_embedding = np.mean(model[first], axis=0)\n",
    "second_sent_embedding = np.mean(model[second], axis=0)\n",
    "\n",
    "# calculate similarities\n",
    "result = cosine_similarity(first_sent_embedding.reshape(1,-1),first_sent_embedding.reshape(1,-1))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[0;32m      5\u001b[0m rouge_model \u001b[38;5;241m=\u001b[39m Rouge()\n\u001b[1;32m----> 6\u001b[0m meteor \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeteor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "nltk.translate import meteor\n",
    "import time\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge_model = Rouge()\n",
    "meteor = evaluate.load('meteor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(response):\n",
    "    ans = response.choices[0].text\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_direct(examples, sleep_time=1.2):\n",
    "    #labels = np.array(examples['label'])\n",
    "    premises = examples['premise']\n",
    "    questions = examples['question']\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"premise\"])):\n",
    "        if examples[\"label\"][i] == 0:\n",
    "            labels.append(examples[\"choice1\"][i])\n",
    "        else:\n",
    "            labels.append(examples[\"choice2\"][i])\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    anss = []\n",
    "    responses = []\n",
    "    for i in range(len(premises)):\n",
    "        example = { \n",
    "            'premise': premises[i],\n",
    "            'question': questions[i],\n",
    "            #'label': labels[i]\n",
    "        }\n",
    "        res = index(example)\n",
    "        ans = generate_prediction(res)\n",
    "        time.sleep(sleep_time)\n",
    "        responses.append(res)\n",
    "        anss.append(ans)\n",
    "        meteor_score = round(meteor([word_tokenize(ans)], [word_tokenize(labels[i])]))\n",
    "        meteor_scores.append()\n",
    "        bleu_score_1 = sentence_bleu(labels[i].split(), ans,weights=(1, 0, 0, 0))\n",
    "        #bleu_score_2 = sentence_bleu(labels[i].split(), ans,weights=(0, 1, 0, 0))\n",
    "        #bleu_score = np.exp(np.log(bleu_score_1) + 0.5*np.log(bleu_score_2))\n",
    "        bleu_scores.append(bleu_score_1)\n",
    "        rouge_score = rouge_model(ans, labels, avg = True)\n",
    "    return bleu_scores, labels, responses, anss, rouge_score, meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00cf52643f837e2ce16dfdcfe02d2b0b in your message.) {\n  \"error\": {\n    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00cf52643f837e2ce16dfdcfe02d2b0b in your message.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00cf52643f837e2ce16dfdcfe02d2b0b in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 06 Dec 2022 20:24:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'Access-Control-Allow-Origin': '*', 'Openai-Organization': 'epfl-146', 'Openai-Processing-Ms': '30742', 'Openai-Version': '2020-10-01', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains', 'X-Request-Id': '00cf52643f837e2ce16dfdcfe02d2b0b'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bleu_scores, labels, responses, anss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopa\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [16], line 20\u001b[0m, in \u001b[0;36mtest_direct\u001b[1;34m(examples, sleep_time)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(premises)):\n\u001b[0;32m     15\u001b[0m     example \u001b[38;5;241m=\u001b[39m { \n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m: premises[i],\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: questions[i],\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m#'label': labels[i]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     }\n\u001b[1;32m---> 20\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     ans \u001b[38;5;241m=\u001b[39m generate_prediction(res)\n\u001b[0;32m     22\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep_time)\n",
      "Cell \u001b[1;32mIn [7], line 2\u001b[0m, in \u001b[0;36mindex\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex\u001b[39m(example):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_prompt_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:115\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    107\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[0;32m    108\u001b[0m     api_key,\n\u001b[0;32m    109\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m--> 115\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_requestor.py:181\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    162\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    171\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    172\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    173\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[1;32m--> 181\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_requestor.py:396\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    390\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    391\u001b[0m         )\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    393\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 396\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_requestor.py:429\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    427\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    430\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    431\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mAPIError\u001b[0m: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00cf52643f837e2ce16dfdcfe02d2b0b in your message.) {\n  \"error\": {\n    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00cf52643f837e2ce16dfdcfe02d2b0b in your message.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00cf52643f837e2ce16dfdcfe02d2b0b in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 06 Dec 2022 20:24:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'Access-Control-Allow-Origin': '*', 'Openai-Organization': 'epfl-146', 'Openai-Processing-Ms': '30742', 'Openai-Version': '2020-10-01', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains', 'X-Request-Id': '00cf52643f837e2ce16dfdcfe02d2b0b'}"
     ]
    }
   ],
   "source": [
    "bleu_scores, labels, responses, anss, rouge_score, meteor_scores = test_direct(copa['validation'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores_average = np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scores_average = np.mean(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_response(r, example, ans):\n",
    "    print(example)\n",
    "    #print(r.choices[0].text)\n",
    "    print(\"Answer :\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The man turned on the faucet.', 'choice1': 'The toilet filled with water.', 'choice2': 'Water flowed from the spout.', 'question': 'effect', 'idx': 0, 'label': 1}\n",
      "Answer : Water began to flow from the faucet.\n",
      "{'premise': 'The girl found a bug in her cereal.', 'choice1': 'She poured milk in the bowl.', 'choice2': 'She lost her appetite.', 'question': 'effect', 'idx': 1, 'label': 1}\n",
      "Answer : She was disgusted and threw out the cereal.\n",
      "{'premise': 'The woman retired.', 'choice1': 'She received her pension.', 'choice2': 'She paid off her mortgage.', 'question': 'effect', 'idx': 2, 'label': 0}\n",
      "Answer : She stopped working and began to enjoy her free time.\n",
      "{'premise': 'I wanted to conserve energy.', 'choice1': 'I swept the floor in the unoccupied room.', 'choice2': 'I shut off the light in the unoccupied room.', 'question': 'effect', 'idx': 3, 'label': 1}\n",
      "Answer : I turned off the lights when I left the room.\n",
      "{'premise': 'The hamburger meat browned.', 'choice1': 'The cook froze it.', 'choice2': 'The cook grilled it.', 'question': 'cause', 'idx': 4, 'label': 1}\n",
      "Answer : It was cooked over heat.\n",
      "{'premise': \"I doubted the salesman's pitch.\", 'choice1': 'I turned his offer down.', 'choice2': 'He persuaded me to buy the product.', 'question': 'effect', 'idx': 5, 'label': 0}\n",
      "Answer : I was hesitant to purchase the product.\n",
      "{'premise': 'I decided to stay home for the night.', 'choice1': 'The forecast called for storms.', 'choice2': 'My friends urged me to go out.', 'question': 'cause', 'idx': 6, 'label': 0}\n",
      "Answer : I was feeling tired and didn't want to go out.\n",
      "{'premise': 'My eyes became red and puffy.', 'choice1': 'I was sobbing.', 'choice2': 'I was laughing.', 'question': 'cause', 'idx': 7, 'label': 0}\n",
      "Answer : I was crying.\n",
      "{'premise': 'The flame on the candle went out.', 'choice1': 'I blew on the wick.', 'choice2': 'I put a match to the wick.', 'question': 'cause', 'idx': 8, 'label': 0}\n",
      "Answer : The candle burned down or the wind blew it out.\n",
      "{'premise': 'The man drank heavily at the party.', 'choice1': 'He had a headache the next day.', 'choice2': 'He had a runny nose the next day.', 'question': 'effect', 'idx': 9, 'label': 0}\n",
      "Answer : He became intoxicated.\n",
      "{'premise': 'The bowling ball knocked over the bowling pins.', 'choice1': 'The man rolled the bowling ball down the alley.', 'choice2': 'The man dropped the bowling ball on his foot.', 'question': 'cause', 'idx': 10, 'label': 0}\n",
      "Answer : The bowling ball was thrown.\n",
      "{'premise': \"The community learned of the man's death.\", 'choice1': 'His family buried him in the cemetery.', 'choice2': 'His obituary appeared in the newspaper.', 'question': 'cause', 'idx': 11, 'label': 1}\n",
      "Answer : He passed away.\n",
      "{'premise': 'My computer crashed.', 'choice1': 'I installed new speakers.', 'choice2': 'I lost all my data.', 'question': 'effect', 'idx': 12, 'label': 1}\n",
      "Answer : I lost all of my unsaved work.\n",
      "{'premise': 'The woman resigned from her job.', 'choice1': 'She aspired to hold an executive position in the firm.', 'choice2': 'She believed her superiors were acting unethically.', 'question': 'cause', 'idx': 13, 'label': 1}\n",
      "Answer : She was unhappy with the working conditions.\n",
      "{'premise': 'The player caught the ball.', 'choice1': 'Her teammate threw it to her.', 'choice2': 'Her opponent tried to intercept it.', 'question': 'cause', 'idx': 14, 'label': 0}\n",
      "Answer : The ball was thrown to him.\n",
      "{'premise': 'The judge pounded the gavel.', 'choice1': 'The courtroom broke into uproar.', 'choice2': 'The jury announced its verdict.', 'question': 'cause', 'idx': 15, 'label': 0}\n",
      "Answer : The judge wanted to restore order in the court.\n",
      "{'premise': 'The woman banished the children from her property.', 'choice1': 'The children hit a ball into her yard.', 'choice2': 'The children trampled through her garden.', 'question': 'cause', 'idx': 16, 'label': 1}\n",
      "Answer : The children were playing too loudly.\n",
      "{'premise': 'The kidnappers released the hostage.', 'choice1': 'They accepted ransom money.', 'choice2': 'They escaped from jail.', 'question': 'cause', 'idx': 17, 'label': 0}\n",
      "Answer : The police pressured them to do so.\n",
      "{'premise': \"The cook's eyes watered.\", 'choice1': 'He ran out of onions.', 'choice2': 'He cut an onion.', 'question': 'cause', 'idx': 18, 'label': 1}\n",
      "Answer : The smoke from the oven was getting in his eyes.\n",
      "{'premise': 'The woman ran her finger under cold water.', 'choice1': 'She burned her finger on the toaster.', 'choice2': 'She put a diamond ring on her finger.', 'question': 'cause', 'idx': 19, 'label': 0}\n",
      "Answer : She burned her finger on a hot pan.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    show_example_response(responses[i], copa['validation'][i], anss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "578317ea115a054ac4bccdfa892faa859649678dbaf6df54b2b98fb2846a81cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
