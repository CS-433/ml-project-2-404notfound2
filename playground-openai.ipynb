{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Causal Reasoning Using Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "openai.api_key_path=\"api_key.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COPA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (C:/Users/gazer/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
      "100%|██████████| 3/3 [00:00<00:00, 601.59it/s]\n"
     ]
    }
   ],
   "source": [
    "copa = load_dataset(\"super_glue\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'My body cast a shadow over the grass.',\n",
       " 'choice1': 'The sun was rising.',\n",
       " 'choice2': 'The grass was cut.',\n",
       " 'question': 'cause',\n",
       " 'idx': 0,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See one example\n",
    "copa[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "print(len(copa[\"train\"]))\n",
    "print(len(copa[\"validation\"]))\n",
    "print(len(copa[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "using the whole set directly as a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_base(example):\n",
    "    return \"\"\" Identify the correct response from two sentences.\n",
    "    \n",
    "    Premise: {}\n",
    "    Choice1: {}\n",
    "    Choice2: {}\n",
    "    Question: {}\n",
    "    Answer:\n",
    "    \"\"\".format(\n",
    "        example['premise'],\n",
    "        example['choice1'],\n",
    "        example['choice2'],\n",
    "        example['question'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(example):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=generate_prompt_base(example),\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_label(choices, response):\n",
    "    ans = response.choices[0].text\n",
    "    ans = ans.replace(\"Choice1:\", \"\")\n",
    "    ans = ans.replace(\"Choice2:\", \"\")\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    if (ans==choices[0]):\n",
    "        return 0, ans\n",
    "    elif (ans==choices[1]):\n",
    "        return 1, ans\n",
    "    else: \n",
    "        return -1, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(examples, sleep_time=0.5):\n",
    "    labels = np.array(examples['label'])\n",
    "    premises = examples['premise']\n",
    "    choice1s = examples['choice1']\n",
    "    choice2s = examples['choice2']\n",
    "    questions = examples['question']\n",
    "    y_pred = np.zeros(len(labels))\n",
    "    \n",
    "    anss = []\n",
    "    responses = []\n",
    "    for i in range(len(labels)):\n",
    "        example = { \n",
    "            'premise': premises[i],\n",
    "            'choice1': choice1s[i],\n",
    "            'choice2':choice2s[i],\n",
    "            'question': questions[i],\n",
    "        }\n",
    "        res = index(example)\n",
    "        choices = [choice1s[i], choice2s[i]]\n",
    "        y_pred[i], ans = generate_pred_label(choices, res)\n",
    "        time.sleep(sleep_time)\n",
    "        responses.append(res)\n",
    "        anss.append(ans)\n",
    "    return y_pred, labels, responses, anss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true, responses, anss= test(copa['validation'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_response(r, example, ans):\n",
    "    print(example)\n",
    "    print(r.choices[0].text)\n",
    "    print(\"Answer :\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The man turned on the faucet.', 'choice1': 'The toilet filled with water.', 'choice2': 'Water flowed from the spout.', 'question': 'effect', 'idx': 0, 'label': 1}\n",
      "\n",
      "Choice2: Water flowed from the spout.\n",
      "Answer : Water flowed from the spout.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'The girl found a bug in her cereal.', 'choice1': 'She poured milk in the bowl.', 'choice2': 'She lost her appetite.', 'question': 'effect', 'idx': 1, 'label': 1}\n",
      " She lost her appetite.\n",
      "Answer : She lost her appetite.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'The woman retired.', 'choice1': 'She received her pension.', 'choice2': 'She paid off her mortgage.', 'question': 'effect', 'idx': 2, 'label': 0}\n",
      " She received her pension.\n",
      "Answer : She received her pension.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'I wanted to conserve energy.', 'choice1': 'I swept the floor in the unoccupied room.', 'choice2': 'I shut off the light in the unoccupied room.', 'question': 'effect', 'idx': 3, 'label': 1}\n",
      " I shut off the light in the unoccupied room.\n",
      "Answer : I shut off the light in the unoccupied room.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'The hamburger meat browned.', 'choice1': 'The cook froze it.', 'choice2': 'The cook grilled it.', 'question': 'cause', 'idx': 4, 'label': 1}\n",
      " Choice2: The cook grilled it.\n",
      "Answer : The cook grilled it.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    show_example_response(responses[i], copa['validation'][i], anss[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline on validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = np.zeros(len(copa['validation']))\n",
    "val_true = np.zeros(len(copa['validation']))\n",
    "val_responses = []\n",
    "val_ans = []\n",
    "for i in range(5):\n",
    "    a = i*20\n",
    "    b = (i+1)*20\n",
    "    pred, true, responses, anss= test(copa['validation'][a:b], sleep_time = 1)\n",
    "    val_pred[a:b] = pred\n",
    "    val_true[a:b] = true\n",
    "    val_responses.append(responses)\n",
    "    val_ans.append(anss)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.96      0.93        55\n",
      "         1.0       0.95      0.87      0.91        45\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.92      0.92      0.92       100\n",
      "weighted avg       0.92      0.92      0.92       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_true, val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
