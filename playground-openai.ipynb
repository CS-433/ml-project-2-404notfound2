{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Causal Reasoning Using Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "openai.api_key_path=\"api_key.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COPA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b3fd45515a40318cdcb6df556539be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "copa = load_dataset(\"super_glue\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'My body cast a shadow over the grass.',\n",
       " 'choice1': 'The sun was rising.',\n",
       " 'choice2': 'The grass was cut.',\n",
       " 'question': 'cause',\n",
       " 'idx': 0,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See one example\n",
    "copa[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "print(len(copa[\"train\"]))\n",
    "print(len(copa[\"validation\"]))\n",
    "print(len(copa[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "using the whole set directly as a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_base(example):\n",
    "    return \"\"\" Identify the correct response from two sentences.\n",
    "    \n",
    "    Premise: {}\n",
    "    Choice1: {}\n",
    "    Choice2: {}\n",
    "    Question: {}\n",
    "    Answer:\n",
    "    \"\"\".format(\n",
    "        example['premise'],\n",
    "        example['choice1'],\n",
    "        example['choice2'],\n",
    "        example['question'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(example):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=generate_prompt_base(example),\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_label(choices, response):\n",
    "    ans = response.choices[0].text\n",
    "    ans = ans.replace(\"Choice1:\", \"\")\n",
    "    ans = ans.replace(\"Choice2:\", \"\")\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    if (ans==choices[0]):\n",
    "        return 0, ans\n",
    "    elif (ans==choices[1]):\n",
    "        return 1, ans\n",
    "    else: \n",
    "        return 1, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(examples, sleep_time=0.5):\n",
    "    labels = np.array(examples['label'])\n",
    "    premises = examples['premise']\n",
    "    choice1s = examples['choice1']\n",
    "    choice2s = examples['choice2']\n",
    "    questions = examples['question']\n",
    "    y_pred = np.zeros(len(labels))\n",
    "    \n",
    "    anss = []\n",
    "    responses = []\n",
    "    for i in range(len(labels)):\n",
    "        example = { \n",
    "            'premise': premises[i],\n",
    "            'choice1': choice1s[i],\n",
    "            'choice2':choice2s[i],\n",
    "            'question': questions[i],\n",
    "        }\n",
    "        res = index(example)\n",
    "        choices = [choice1s[i], choice2s[i]]\n",
    "        y_pred[i], ans = generate_pred_label(choices, res)\n",
    "        time.sleep(sleep_time)\n",
    "        responses.append(res)\n",
    "        anss.append(ans)\n",
    "    return y_pred, labels, responses, anss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true, responses, anss= test(copa['validation'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_response(r, example, ans):\n",
    "    print(example)\n",
    "    print(r.choices[0].text)\n",
    "    print(\"Answer :\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The man turned on the faucet.', 'choice1': 'The toilet filled with water.', 'choice2': 'Water flowed from the spout.', 'question': 'effect', 'idx': 0, 'label': 1}\n",
      "\n",
      "Choice2: Water flowed from the spout.\n",
      "Answer : Water flowed from the spout.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'The girl found a bug in her cereal.', 'choice1': 'She poured milk in the bowl.', 'choice2': 'She lost her appetite.', 'question': 'effect', 'idx': 1, 'label': 1}\n",
      " She lost her appetite.\n",
      "Answer : She lost her appetite.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'The woman retired.', 'choice1': 'She received her pension.', 'choice2': 'She paid off her mortgage.', 'question': 'effect', 'idx': 2, 'label': 0}\n",
      " She received her pension.\n",
      "Answer : She received her pension.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'I wanted to conserve energy.', 'choice1': 'I swept the floor in the unoccupied room.', 'choice2': 'I shut off the light in the unoccupied room.', 'question': 'effect', 'idx': 3, 'label': 1}\n",
      " I shut off the light in the unoccupied room.\n",
      "Answer : I shut off the light in the unoccupied room.\n",
      "\n",
      "\n",
      "\n",
      "{'premise': 'The hamburger meat browned.', 'choice1': 'The cook froze it.', 'choice2': 'The cook grilled it.', 'question': 'cause', 'idx': 4, 'label': 1}\n",
      " Choice2: The cook grilled it.\n",
      "Answer : The cook grilled it.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    show_example_response(responses[i], copa['validation'][i], anss[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline on validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, p, r, f = [], [], [], []\n",
    "for run_time in range(5):\n",
    "    val_pred = np.zeros(len(copa['validation']))\n",
    "    val_true = np.zeros(len(copa['validation']))\n",
    "    val_responses = []\n",
    "    val_ans = []\n",
    "    for i in range(5):\n",
    "        st = i*20\n",
    "        ed = (i+1)*20\n",
    "        pred, true, responses, anss= test(copa['validation'][st:ed], sleep_time = 3)\n",
    "        # time.sleep(2)\n",
    "        val_pred[st:ed] = pred\n",
    "        val_true[st:ed] = true\n",
    "        val_responses.append(responses)\n",
    "        val_ans.append(anss)\n",
    "    a.append(accuracy.compute(predictions = val_pred, references = val_true)[\"accuracy\"])\n",
    "    p.append(precision.compute(predictions = val_pred, references = val_true, average=\"macro\")[\"precision\"])\n",
    "    r.append(recall.compute(predictions = val_pred, references = val_true, average=\"macro\")[\"recall\"])\n",
    "    f.append(f1.compute(predictions = val_pred, references = val_true, average=\"macro\")[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.96      0.93        55\n",
      "         1.0       0.95      0.87      0.91        45\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.92      0.92      0.92       100\n",
      "weighted avg       0.92      0.92      0.92       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_true, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9200000000000002 0.006324555320336764\n",
      "0.9239870999361243 0.006559787750021689\n",
      "0.9155555555555555 0.006407576374607563\n",
      "0.9184902950437671 0.006434214461826098\n"
     ]
    }
   ],
   "source": [
    "print(np.array(a).mean(), np.array(a).std())\n",
    "print(np.array(p).mean(), np.array(p).std())\n",
    "print(np.array(r).mean(), np.array(r).std())\n",
    "print(np.array(f).mean(), np.array(f).std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2115e6518f0deb2150f8874acb24f1345316df487d5eca9760b49a6975aa76aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
