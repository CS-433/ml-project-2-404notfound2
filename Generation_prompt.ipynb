{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Causal Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COPA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (C:/Users/lyh/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
      "100%|██████████| 3/3 [00:00<00:00, 601.65it/s]\n"
     ]
    }
   ],
   "source": [
    "copa = load_dataset(\"super_glue\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'I drank from the water fountain.',\n",
       " 'choice1': 'I was thirsty.',\n",
       " 'choice2': 'I felt nauseous.',\n",
       " 'question': 'cause',\n",
       " 'idx': 50,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See one example\n",
    "\n",
    "copa[\"train\"][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "print(len(copa[\"train\"]))\n",
    "print(len(copa[\"validation\"]))\n",
    "print(len(copa[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "#For different users, you need to replace the api_key to your own in api_key.txt\n",
    "openai.api_key_path=\"api_key.txt\"\n",
    "#Or directly use api_key below\n",
    "#openai.api_key = \"your_own_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the promt model(Instruction + 20 exapmles + Task)\n",
    "def generate_prompt_direct(example):\n",
    "    return \"\"\" Answer the Question of Premise.\n",
    "    Premise:The teacher took roll.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:She identified the students that were absent.\n",
    "    \n",
    "    Premise:The book was deemed inappropriate for children.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:Schools banned it from its libraries.\n",
    "    \n",
    "    Premise:The child caught a life-threatening illness.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She didn't get vaccinated.\n",
    "    \n",
    "    Premise:The young woman was denied entrance into the bar.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She forgot her ID.\n",
    "    \n",
    "    Premise:The man had lipstick on his cheek.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:The woman kissed him.\n",
    "    \n",
    "    Premise:The woman had a religious awakening.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:She began going to church.\n",
    "    \n",
    "    Premise:The woman deleted the email.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:The sender tried to solicit money from her.\n",
    "    \n",
    "    Premise:The girl wanted to make her mother happy.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The girl picked a flower for her mother.\n",
    "    \n",
    "    Premise:The girl applied the scissors to the paper.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The paper sliced apart.\n",
    "    \n",
    "    Premise:I got distracted from my conversation with the woman.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:Everyone in the room was talking.\n",
    "    \n",
    "    Premise:The cat purred.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:I petted it.\n",
    "    \n",
    "    Premise:The police officer pulled over the celebrity.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The celebrity offered the officer a bribe.\n",
    "    \n",
    "    Premise:My feet were blistered.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:I went hiking.\n",
    "    \n",
    "    Premise:The shopper wondered about the cost of the item.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:He checked its price tag.\n",
    "    \n",
    "    Premise:The woman wrote a check to the gas company.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She received her monthly bill.\n",
    "    \n",
    "    Premise:I applied pressure to the cut on my arm.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:It stopped bleeding.\n",
    "    \n",
    "    Premise:The man needed coins to fill the parking meter.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:He searched under his car seats for loose change.\n",
    "    \n",
    "    Premise:I lingered in bed upon awakening.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:It was Saturday.\n",
    "    \n",
    "    Premise:I put ice cubes in the hot soup.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The soup cooled down.\n",
    "    \n",
    "    Premise:The patient underwent the risky medical procedure.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:Specialists recommended the procedure.\n",
    "    \n",
    "    Premise: {}\n",
    "    Question:What is the {} of Premise?\n",
    "    Answer:\n",
    "    \n",
    "    \"\"\".format(\n",
    "        example['premise'],\n",
    "        example['question'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response model parameters\n",
    "def index(example):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=generate_prompt_direct(example),\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "import time\n",
    "from torchmetrics.functional.text.rouge import rouge_score\n",
    "from cider.cider import Cider\n",
    "\n",
    "cider = Cider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(response):\n",
    "    ans = response.choices[0].text\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test function\n",
    "def test_direct(examples, sleep_time=1):\n",
    "    \n",
    "    '''\n",
    "    Testing the model of prompt learning\n",
    "    Args: examples\n",
    "    \n",
    "    Returns: Lists including metrics computed in every datapoints: \n",
    "             bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4,\n",
    "             labels, responses, anss, rouge_scores, meteor_scores, cider_score\n",
    "    \n",
    "    '''\n",
    "    premises = examples['premise']\n",
    "    questions = examples['question']\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"premise\"])):\n",
    "        if examples[\"label\"][i] == 0:\n",
    "            labels.append(examples[\"choice1\"][i])\n",
    "        else:\n",
    "            labels.append(examples[\"choice2\"][i])\n",
    "    \n",
    "    bleu_scores_1 = []\n",
    "    bleu_scores_2 = []\n",
    "    bleu_scores_3 = []\n",
    "    bleu_scores_4 = []\n",
    "    meteor_scores = []\n",
    "    anss = []\n",
    "    rouge_scores = [] \n",
    "    responses = []\n",
    "    for i in range(len(premises)):\n",
    "        example = { \n",
    "            'premise': premises[i],\n",
    "            'question': questions[i],\n",
    "        }\n",
    "        res = index(example)\n",
    "        ans = generate_prediction(res)\n",
    "        time.sleep(sleep_time)\n",
    "        responses.append(res)\n",
    "        anss.append(ans)\n",
    "        meteor_score = round(meteor([word_tokenize(ans)], word_tokenize(labels[i])))\n",
    "        meteor_scores.append(meteor_score)\n",
    "        bleu_score_4 = sentence_bleu(labels[i].split(), ans)\n",
    "        bleu_score_3 = sentence_bleu(labels[i].split(), ans,weights=(0, 0, 1, 0))\n",
    "        bleu_score_2 = sentence_bleu(labels[i].split(), ans,weights=(0, 1, 0, 0))\n",
    "        bleu_score_1 = sentence_bleu(labels[i].split(), ans,weights=(1, 0, 0, 0))\n",
    "        bleu_scores_1.append(bleu_score_1)\n",
    "        bleu_scores_2.append(bleu_score_2)\n",
    "        bleu_scores_3.append(bleu_score_3)\n",
    "        bleu_scores_4.append(bleu_score_4)\n",
    "        rouge_ = rouge_score(ans, labels[i])\n",
    "        rouge_scores.append(rouge_)\n",
    "    cider_score = cider.compute(anss, [labels])\n",
    "    return bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels, responses, anss, rouge_scores, meteor_scores, cider_score# rouge_score# meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels,responses, anss, rouge_scores, meteor_scores,cider_score = test_direct(copa['validation'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores1_average = np.mean(bleu_scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3984325808408321"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores1_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3984325808408321\n",
      "0.23815132012019802\n",
      "0.1468892222838526\n",
      "0.1514231753999145\n"
     ]
    }
   ],
   "source": [
    "bleu_scores1_average = np.mean(bleu_scores_1)\n",
    "bleu_scores2_average = np.mean(bleu_scores_2)\n",
    "bleu_scores3_average = np.mean(bleu_scores_3)\n",
    "bleu_scores4_average = np.mean(bleu_scores_4)\n",
    "print(bleu_scores1_average)\n",
    "print(bleu_scores2_average)\n",
    "print(bleu_scores3_average)\n",
    "print(bleu_scores4_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scores_average = np.mean(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14061905"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_list = []\n",
    "for rouge_ in rouge_scores:\n",
    "    rouge_list.append(rouge_['rouge2_recall'])\n",
    "rouge_average = np.mean(rouge_list)\n",
    "rouge_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_response(r, example, ans):\n",
    "    \n",
    "    print(example)\n",
    "    print(\"Answer :\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The man turned on the faucet.', 'choice1': 'The toilet filled with water.', 'choice2': 'Water flowed from the spout.', 'question': 'effect', 'idx': 0, 'label': 1}\n",
      "Answer : Water began flowing out of the faucet.\n",
      "{'premise': 'The girl found a bug in her cereal.', 'choice1': 'She poured milk in the bowl.', 'choice2': 'She lost her appetite.', 'question': 'effect', 'idx': 1, 'label': 1}\n",
      "Answer : She threw the cereal away.\n",
      "{'premise': 'The woman retired.', 'choice1': 'She received her pension.', 'choice2': 'She paid off her mortgage.', 'question': 'effect', 'idx': 2, 'label': 0}\n",
      "Answer : She stopped working and began collecting her pension.\n",
      "{'premise': 'I wanted to conserve energy.', 'choice1': 'I swept the floor in the unoccupied room.', 'choice2': 'I shut off the light in the unoccupied room.', 'question': 'effect', 'idx': 3, 'label': 1}\n",
      "Answer : I turned off the lights when I left the room.\n",
      "{'premise': 'The hamburger meat browned.', 'choice1': 'The cook froze it.', 'choice2': 'The cook grilled it.', 'question': 'cause', 'idx': 4, 'label': 1}\n",
      "Answer : It was cooked on a hot stove.\n",
      "{'premise': \"I doubted the salesman's pitch.\", 'choice1': 'I turned his offer down.', 'choice2': 'He persuaded me to buy the product.', 'question': 'effect', 'idx': 5, 'label': 0}\n",
      "Answer : I refused to buy the product.\n",
      "{'premise': 'I decided to stay home for the night.', 'choice1': 'The forecast called for storms.', 'choice2': 'My friends urged me to go out.', 'question': 'cause', 'idx': 6, 'label': 0}\n",
      "Answer : I was feeling too tired to go out.\n",
      "{'premise': 'My eyes became red and puffy.', 'choice1': 'I was sobbing.', 'choice2': 'I was laughing.', 'question': 'cause', 'idx': 7, 'label': 0}\n",
      "Answer : I was crying.\n",
      "{'premise': 'The flame on the candle went out.', 'choice1': 'I blew on the wick.', 'choice2': 'I put a match to the wick.', 'question': 'cause', 'idx': 8, 'label': 0}\n",
      "Answer : The candle burned down and the wick was extinguished.\n",
      "{'premise': 'The man drank heavily at the party.', 'choice1': 'He had a headache the next day.', 'choice2': 'He had a runny nose the next day.', 'question': 'effect', 'idx': 9, 'label': 0}\n",
      "Answer : He became intoxicated.\n",
      "{'premise': 'The bowling ball knocked over the bowling pins.', 'choice1': 'The man rolled the bowling ball down the alley.', 'choice2': 'The man dropped the bowling ball on his foot.', 'question': 'cause', 'idx': 10, 'label': 0}\n",
      "Answer : The bowling ball was thrown down the lane.\n",
      "{'premise': \"The community learned of the man's death.\", 'choice1': 'His family buried him in the cemetery.', 'choice2': 'His obituary appeared in the newspaper.', 'question': 'cause', 'idx': 11, 'label': 1}\n",
      "Answer : The man passed away.\n",
      "{'premise': 'My computer crashed.', 'choice1': 'I installed new speakers.', 'choice2': 'I lost all my data.', 'question': 'effect', 'idx': 12, 'label': 1}\n",
      "Answer : I had to restart it.\n",
      "{'premise': 'The woman resigned from her job.', 'choice1': 'She aspired to hold an executive position in the firm.', 'choice2': 'She believed her superiors were acting unethically.', 'question': 'cause', 'idx': 13, 'label': 1}\n",
      "Answer : She was unhappy with her work environment.\n",
      "{'premise': 'The player caught the ball.', 'choice1': 'Her teammate threw it to her.', 'choice2': 'Her opponent tried to intercept it.', 'question': 'cause', 'idx': 14, 'label': 0}\n",
      "Answer : The ball was thrown in his direction.\n",
      "{'premise': 'The judge pounded the gavel.', 'choice1': 'The courtroom broke into uproar.', 'choice2': 'The jury announced its verdict.', 'question': 'cause', 'idx': 15, 'label': 0}\n",
      "Answer : The judge wanted to restore order in the court.\n",
      "{'premise': 'The woman banished the children from her property.', 'choice1': 'The children hit a ball into her yard.', 'choice2': 'The children trampled through her garden.', 'question': 'cause', 'idx': 16, 'label': 1}\n",
      "Answer : The children were playing too loudly.\n",
      "{'premise': 'The kidnappers released the hostage.', 'choice1': 'They accepted ransom money.', 'choice2': 'They escaped from jail.', 'question': 'cause', 'idx': 17, 'label': 0}\n",
      "Answer : The kidnappers received their ransom.\n",
      "{'premise': \"The cook's eyes watered.\", 'choice1': 'He ran out of onions.', 'choice2': 'He cut an onion.', 'question': 'cause', 'idx': 18, 'label': 1}\n",
      "Answer : The cook was cutting onions.\n",
      "{'premise': 'The woman ran her finger under cold water.', 'choice1': 'She burned her finger on the toaster.', 'choice2': 'She put a diamond ring on her finger.', 'question': 'cause', 'idx': 19, 'label': 0}\n",
      "Answer : She burned her finger on a hot pot.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    show_example_response(responses[i], copa['validation'][i], anss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "578317ea115a054ac4bccdfa892faa859649678dbaf6df54b2b98fb2846a81cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
