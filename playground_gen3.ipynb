{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Causal Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COPA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (C:/Users/lyh/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
      "100%|██████████| 3/3 [00:00<00:00, 601.65it/s]\n"
     ]
    }
   ],
   "source": [
    "copa = load_dataset(\"super_glue\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'I drank from the water fountain.',\n",
       " 'choice1': 'I was thirsty.',\n",
       " 'choice2': 'I felt nauseous.',\n",
       " 'question': 'cause',\n",
       " 'idx': 50,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See one example\n",
    "\n",
    "copa[\"train\"][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "print(len(copa[\"train\"]))\n",
    "print(len(copa[\"validation\"]))\n",
    "print(len(copa[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "See [Multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice).\n",
    "\n",
    "Here, we use `AutoModelForMultipleChoice` for the baseline. The model receives one input sentence as question and several sentences as candidates. Then the model predicts the correct answer sentence by text classification. Here we use `premise` with `question` as query and choice_i as candidates.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "- `query`: my body cast a shadow over the grass because\n",
    "- `candidates1`: the sun was rising.\n",
    "- `candidates2`: the grass was cut.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n",
    "\n",
    "- `query`: the elderly woman suffered a stroke so\n",
    "- `candidates1`: the woman's daughter came over to clean her house.\n",
    "- `candidates2`: the woman's daughter moved in to take care of her."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "See [Multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice).\n",
    "\n",
    "Here, we use `AutoModelForMultipleChoice` for the baseline. The model receives one input sentence as question and several sentences as candidates. Then the model predicts the correct answer sentence by text classification. Here we use `premise` with `question` as query and choice_i as candidates.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "- `query`: my body cast a shadow over the grass because\n",
    "- `candidates1`: the sun was rising.\n",
    "- `candidates2`: the grass was cut.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n",
    "\n",
    "- `query`: the elderly woman suffered a stroke so\n",
    "- `candidates1`: the woman's daughter came over to clean her house.\n",
    "- `candidates2`: the woman's daughter moved in to take care of her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_direct(example):\n",
    "    return \"\"\" Answer the Question of Premise.\n",
    "    Premise:The teacher took roll.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:She identified the students that were absent.\n",
    "    \n",
    "    Premise:The book was deemed inappropriate for children.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:Schools banned it from its libraries.\n",
    "    \n",
    "    Premise:The child caught a life-threatening illness.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She didn't get vaccinated.\n",
    "    \n",
    "    Premise:The young woman was denied entrance into the bar.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She forgot her ID.\n",
    "    \n",
    "    Premise:The man had lipstick on his cheek.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:The woman kissed him.\n",
    "    \n",
    "    Premise:The woman had a religious awakening.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:She began going to church.\n",
    "    \n",
    "    Premise:The woman deleted the email.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:The sender tried to solicit money from her.\n",
    "    \n",
    "    Premise:The girl wanted to make her mother happy.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The girl picked a flower for her mother.\n",
    "    \n",
    "    Premise:The girl applied the scissors to the paper.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The paper sliced apart.\n",
    "    \n",
    "    Premise:I got distracted from my conversation with the woman.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:Everyone in the room was talking.\n",
    "    \n",
    "    Premise:The cat purred.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:I petted it.\n",
    "    \n",
    "    Premise:The police officer pulled over the celebrity.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The celebrity offered the officer a bribe.\n",
    "    \n",
    "    Premise:My feet were blistered.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:I went hiking.\n",
    "    \n",
    "    Premise:The shopper wondered about the cost of the item.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:He checked its price tag.\n",
    "    \n",
    "    Premise:The woman wrote a check to the gas company.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She received her monthly bill.\n",
    "    \n",
    "    Premise:I applied pressure to the cut on my arm.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:It stopped bleeding.\n",
    "    \n",
    "    Premise:The man needed coins to fill the parking meter.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:He searched under his car seats for loose change.\n",
    "    \n",
    "    Premise:I lingered in bed upon awakening.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:It was Saturday.\n",
    "    \n",
    "    Premise:I put ice cubes in the hot soup.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The soup cooled down.\n",
    "    \n",
    "    Premise:I ran the ice cube under warm water.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The ice cube vanished.2\n",
    "    \n",
    "    Premise:The patient underwent the risky medical procedure.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:Specialists recommended the procedure.\n",
    "    \n",
    "    Premise: {}\n",
    "    Question:What is the {} of Premise?\n",
    "    Answer:\n",
    "    \n",
    "    \"\"\".format(\n",
    "        example['premise'],\n",
    "        example['question'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'The patient underwent the risky medical procedure.',\n",
       " 'choice1': 'The procedure was costly.',\n",
       " 'choice2': 'Specialists recommended the procedure.',\n",
       " 'question': 'cause',\n",
       " 'idx': 360,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa[\"train\"][360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(example):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=generate_prompt_direct(example),\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "import time\n",
    "from torchmetrics.functional.text.rouge import rouge_score\n",
    "from cider.cider import Cider\n",
    "\n",
    "cider = Cider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(response):\n",
    "    ans = response.choices[0].text\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_direct(examples, sleep_time=1):\n",
    "    #labels = np.array(examples['label'])\n",
    "    premises = examples['premise']\n",
    "    questions = examples['question']\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"premise\"])):\n",
    "        if examples[\"label\"][i] == 0:\n",
    "            labels.append(examples[\"choice1\"][i])\n",
    "        else:\n",
    "            labels.append(examples[\"choice2\"][i])\n",
    "    bleu_scores = []\n",
    "    bleu_scores_1 = []\n",
    "    bleu_scores_2 = []\n",
    "    bleu_scores_3 = []\n",
    "    bleu_scores_4 = []\n",
    "    meteor_scores = []\n",
    "    anss = []\n",
    "    rouge_scores = [] \n",
    "    responses = []\n",
    "    for i in range(len(premises)):\n",
    "        example = { \n",
    "            'premise': premises[i],\n",
    "            'question': questions[i],\n",
    "        }\n",
    "        res = index(example)\n",
    "        ans = generate_prediction(res)\n",
    "        time.sleep(sleep_time)\n",
    "        responses.append(res)\n",
    "        anss.append(ans)\n",
    "        meteor_score = round(meteor([word_tokenize(ans)], word_tokenize(labels[i])))\n",
    "        meteor_scores.append(meteor_score)\n",
    "        bleu_score_4 = sentence_bleu(labels[i].split(), ans)#,weights=(1, 0, 0, 0))\n",
    "        bleu_score_3 = sentence_bleu(labels[i].split(), ans,weights=(0, 0, 1, 0))\n",
    "        bleu_score_2 = sentence_bleu(labels[i].split(), ans,weights=(0, 1, 0, 0))\n",
    "        bleu_score_1 = sentence_bleu(labels[i].split(), ans,weights=(1, 0, 0, 0))\n",
    "        bleu_score = np.exp(np.log(bleu_score_1) + 0.5*np.log(bleu_score_2) + 1/3*np.log(bleu_score_3)+0.25*np.log(bleu_score_4))\n",
    "        bleu_scores.append(bleu_score)\n",
    "        bleu_scores_1.append(bleu_score_1)\n",
    "        bleu_scores_2.append(bleu_score_2)\n",
    "        bleu_scores_3.append(bleu_score_3)\n",
    "        bleu_scores_4.append(bleu_score_4)\n",
    "        rouge_ = rouge_score(ans, labels[i])\n",
    "        rouge_scores.append(rouge_)\n",
    "    cider_score = cider.compute(anss, [labels])\n",
    "    return bleu_scores,bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels, responses, anss, rouge_scores, meteor_scores, cider_score# rouge_score# meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bleu_scores,bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels,responses, anss, rouge_scores, meteor_scores,cider_score \u001b[38;5;241m=\u001b[39m \u001b[43mtest_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopa\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [28], line 25\u001b[0m, in \u001b[0;36mtest_direct\u001b[1;34m(examples, sleep_time)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(premises)):\n\u001b[0;32m     21\u001b[0m     example \u001b[38;5;241m=\u001b[39m { \n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m: premises[i],\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: questions[i],\n\u001b[0;32m     24\u001b[0m     }\n\u001b[1;32m---> 25\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     ans \u001b[38;5;241m=\u001b[39m generate_prediction(res)\n\u001b[0;32m     27\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep_time)\n",
      "Cell \u001b[1;32mIn [25], line 2\u001b[0m, in \u001b[0;36mindex\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex\u001b[39m(example):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_prompt_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:115\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    107\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[0;32m    108\u001b[0m     api_key,\n\u001b[0;32m    109\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m--> 115\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_requestor.py:181\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    162\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    171\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    172\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    173\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[1;32m--> 181\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_requestor.py:396\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    390\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    391\u001b[0m         )\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    393\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 396\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-project2\\lib\\site-packages\\openai\\api_requestor.py:429\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    427\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    430\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    431\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "bleu_scores,bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels,responses, anss, rouge_scores, meteor_scores,cider_score = test_direct(copa['validation'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores_average = np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07022913222912637"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores1_average = np.mean(bleu_scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3984325808408321"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores1_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3984325808408321\n",
      "0.23815132012019802\n",
      "0.1468892222838526\n",
      "0.1514231753999145\n"
     ]
    }
   ],
   "source": [
    "bleu_scores1_average = np.mean(bleu_scores_1)\n",
    "bleu_scores2_average = np.mean(bleu_scores_2)\n",
    "bleu_scores3_average = np.mean(bleu_scores_3)\n",
    "bleu_scores4_average = np.mean(bleu_scores_4)\n",
    "print(bleu_scores1_average)\n",
    "print(bleu_scores2_average)\n",
    "print(bleu_scores3_average)\n",
    "print(bleu_scores4_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scores_average = np.mean(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14061905"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_list = []\n",
    "for rouge_ in rouge_scores:\n",
    "    rouge_list.append(rouge_['rouge2_recall'])\n",
    "rouge_average = np.mean(rouge_list)\n",
    "rouge_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_response(r, example, ans):\n",
    "    print(example)\n",
    "    #print(r.choices[0].text)\n",
    "    print(\"Answer :\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The man turned on the faucet.', 'choice1': 'The toilet filled with water.', 'choice2': 'Water flowed from the spout.', 'question': 'effect', 'idx': 0, 'label': 1}\n",
      "Answer : Water began flowing out of the faucet.\n",
      "{'premise': 'The girl found a bug in her cereal.', 'choice1': 'She poured milk in the bowl.', 'choice2': 'She lost her appetite.', 'question': 'effect', 'idx': 1, 'label': 1}\n",
      "Answer : She threw the cereal away.\n",
      "{'premise': 'The woman retired.', 'choice1': 'She received her pension.', 'choice2': 'She paid off her mortgage.', 'question': 'effect', 'idx': 2, 'label': 0}\n",
      "Answer : She stopped working and began collecting her pension.\n",
      "{'premise': 'I wanted to conserve energy.', 'choice1': 'I swept the floor in the unoccupied room.', 'choice2': 'I shut off the light in the unoccupied room.', 'question': 'effect', 'idx': 3, 'label': 1}\n",
      "Answer : I turned off the lights when I left the room.\n",
      "{'premise': 'The hamburger meat browned.', 'choice1': 'The cook froze it.', 'choice2': 'The cook grilled it.', 'question': 'cause', 'idx': 4, 'label': 1}\n",
      "Answer : It was cooked on a hot stove.\n",
      "{'premise': \"I doubted the salesman's pitch.\", 'choice1': 'I turned his offer down.', 'choice2': 'He persuaded me to buy the product.', 'question': 'effect', 'idx': 5, 'label': 0}\n",
      "Answer : I refused to buy the product.\n",
      "{'premise': 'I decided to stay home for the night.', 'choice1': 'The forecast called for storms.', 'choice2': 'My friends urged me to go out.', 'question': 'cause', 'idx': 6, 'label': 0}\n",
      "Answer : I was feeling too tired to go out.\n",
      "{'premise': 'My eyes became red and puffy.', 'choice1': 'I was sobbing.', 'choice2': 'I was laughing.', 'question': 'cause', 'idx': 7, 'label': 0}\n",
      "Answer : I was crying.\n",
      "{'premise': 'The flame on the candle went out.', 'choice1': 'I blew on the wick.', 'choice2': 'I put a match to the wick.', 'question': 'cause', 'idx': 8, 'label': 0}\n",
      "Answer : The candle burned down and the wick was extinguished.\n",
      "{'premise': 'The man drank heavily at the party.', 'choice1': 'He had a headache the next day.', 'choice2': 'He had a runny nose the next day.', 'question': 'effect', 'idx': 9, 'label': 0}\n",
      "Answer : He became intoxicated.\n",
      "{'premise': 'The bowling ball knocked over the bowling pins.', 'choice1': 'The man rolled the bowling ball down the alley.', 'choice2': 'The man dropped the bowling ball on his foot.', 'question': 'cause', 'idx': 10, 'label': 0}\n",
      "Answer : The bowling ball was thrown down the lane.\n",
      "{'premise': \"The community learned of the man's death.\", 'choice1': 'His family buried him in the cemetery.', 'choice2': 'His obituary appeared in the newspaper.', 'question': 'cause', 'idx': 11, 'label': 1}\n",
      "Answer : The man passed away.\n",
      "{'premise': 'My computer crashed.', 'choice1': 'I installed new speakers.', 'choice2': 'I lost all my data.', 'question': 'effect', 'idx': 12, 'label': 1}\n",
      "Answer : I had to restart it.\n",
      "{'premise': 'The woman resigned from her job.', 'choice1': 'She aspired to hold an executive position in the firm.', 'choice2': 'She believed her superiors were acting unethically.', 'question': 'cause', 'idx': 13, 'label': 1}\n",
      "Answer : She was unhappy with her work environment.\n",
      "{'premise': 'The player caught the ball.', 'choice1': 'Her teammate threw it to her.', 'choice2': 'Her opponent tried to intercept it.', 'question': 'cause', 'idx': 14, 'label': 0}\n",
      "Answer : The ball was thrown in his direction.\n",
      "{'premise': 'The judge pounded the gavel.', 'choice1': 'The courtroom broke into uproar.', 'choice2': 'The jury announced its verdict.', 'question': 'cause', 'idx': 15, 'label': 0}\n",
      "Answer : The judge wanted to restore order in the court.\n",
      "{'premise': 'The woman banished the children from her property.', 'choice1': 'The children hit a ball into her yard.', 'choice2': 'The children trampled through her garden.', 'question': 'cause', 'idx': 16, 'label': 1}\n",
      "Answer : The children were playing too loudly.\n",
      "{'premise': 'The kidnappers released the hostage.', 'choice1': 'They accepted ransom money.', 'choice2': 'They escaped from jail.', 'question': 'cause', 'idx': 17, 'label': 0}\n",
      "Answer : The kidnappers received their ransom.\n",
      "{'premise': \"The cook's eyes watered.\", 'choice1': 'He ran out of onions.', 'choice2': 'He cut an onion.', 'question': 'cause', 'idx': 18, 'label': 1}\n",
      "Answer : The cook was cutting onions.\n",
      "{'premise': 'The woman ran her finger under cold water.', 'choice1': 'She burned her finger on the toaster.', 'choice2': 'She put a diamond ring on her finger.', 'question': 'cause', 'idx': 19, 'label': 0}\n",
      "Answer : She burned her finger on a hot pot.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    show_example_response(responses[i], copa['validation'][i], anss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9 (default, Jun 29 2022, 11:45:57) \n[GCC 8.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
