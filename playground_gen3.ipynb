{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Causal Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COPA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681198afe474eb2803429bec5d23307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "copa = load_dataset(\"super_glue\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'I drank from the water fountain.',\n",
       " 'choice1': 'I was thirsty.',\n",
       " 'choice2': 'I felt nauseous.',\n",
       " 'question': 'cause',\n",
       " 'idx': 50,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See one example\n",
    "\n",
    "copa[\"train\"][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "print(len(copa[\"train\"]))\n",
    "print(len(copa[\"validation\"]))\n",
    "print(len(copa[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "See [Multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice).\n",
    "\n",
    "Here, we use `AutoModelForMultipleChoice` for the baseline. The model receives one input sentence as question and several sentences as candidates. Then the model predicts the correct answer sentence by text classification. Here we use `premise` with `question` as query and choice_i as candidates.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "- `query`: my body cast a shadow over the grass because\n",
    "- `candidates1`: the sun was rising.\n",
    "- `candidates2`: the grass was cut.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n",
    "\n",
    "- `query`: the elderly woman suffered a stroke so\n",
    "- `candidates1`: the woman's daughter came over to clean her house.\n",
    "- `candidates2`: the woman's daughter moved in to take care of her."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "See [Multiple choice](https://huggingface.co/docs/transformers/tasks/multiple_choice).\n",
    "\n",
    "Here, we use `AutoModelForMultipleChoice` for the baseline. The model receives one input sentence as question and several sentences as candidates. Then the model predicts the correct answer sentence by text classification. Here we use `premise` with `question` as query and choice_i as candidates.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "```python\n",
    "{'premise': 'My body cast a shadow over the grass.',\n",
    " 'choice1': 'The sun was rising.',\n",
    " 'choice2': 'The grass was cut.',\n",
    " 'question': 'cause',\n",
    " 'idx': 0,\n",
    " 'label': 0}\n",
    "```\n",
    "\n",
    "- `query`: my body cast a shadow over the grass because\n",
    "- `candidates1`: the sun was rising.\n",
    "- `candidates2`: the grass was cut.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "```python\n",
    "{'premise': 'The elderly woman suffered a stroke.',\n",
    " 'choice1': \"The woman's daughter came over to clean her house.\",\n",
    " 'choice2': \"The woman's daughter moved in to take care of her.\",\n",
    " 'question': 'effect',\n",
    " 'idx': 11,\n",
    " 'label': 1}\n",
    "```\n",
    "\n",
    "- `query`: the elderly woman suffered a stroke so\n",
    "- `candidates1`: the woman's daughter came over to clean her house.\n",
    "- `candidates2`: the woman's daughter moved in to take care of her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'Your key here. Should be secret and not exposed to others.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_direct(example):\n",
    "    return \"\"\" Answer the Question of Premise.\n",
    "    Premise:The teacher took roll.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:She identified the students that were absent.\n",
    "    \n",
    "    Premise:The book was deemed inappropriate for children.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:Schools banned it from its libraries.\n",
    "    \n",
    "    Premise:The child caught a life-threatening illness.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She didn't get vaccinated.\n",
    "    \n",
    "    Premise:The young woman was denied entrance into the bar.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She forgot her ID.\n",
    "    \n",
    "    Premise:The man had lipstick on his cheek.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:The woman kissed him.\n",
    "    \n",
    "    Premise:The woman had a religious awakening.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:She began going to church.\n",
    "    \n",
    "    Premise:The woman deleted the email.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:The sender tried to solicit money from her.\n",
    "    \n",
    "    Premise:The girl wanted to make her mother happy.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The girl picked a flower for her mother.\n",
    "    \n",
    "    Premise:The girl applied the scissors to the paper.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The paper sliced apart.\n",
    "    \n",
    "    Premise:I got distracted from my conversation with the woman.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:Everyone in the room was talking.\n",
    "    \n",
    "    Premise:The cat purred.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:I petted it.\n",
    "    \n",
    "    Premise:The police officer pulled over the celebrity.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The celebrity offered the officer a bribe.\n",
    "    \n",
    "    Premise:My feet were blistered.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:I went hiking.\n",
    "    \n",
    "    Premise:The shopper wondered about the cost of the item.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:He checked its price tag.\n",
    "    \n",
    "    Premise:The woman wrote a check to the gas company.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:She received her monthly bill.\n",
    "    \n",
    "    Premise:I applied pressure to the cut on my arm.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:It stopped bleeding.\n",
    "    \n",
    "    Premise:The man needed coins to fill the parking meter.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:He searched under his car seats for loose change.\n",
    "    \n",
    "    Premise:I lingered in bed upon awakening.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:It was Saturday.\n",
    "    \n",
    "    Premise:I put ice cubes in the hot soup.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The soup cooled down.\n",
    "    \n",
    "    Premise:I ran the ice cube under warm water.\n",
    "    Question:What is the effect of Premise?\n",
    "    Answer:The ice cube vanished.2\n",
    "    \n",
    "    Premise:The patient underwent the risky medical procedure.\n",
    "    Question:What is the cause of Premise?\n",
    "    Answer:Specialists recommended the procedure.\n",
    "    \n",
    "    Premise: {}\n",
    "    Question:What is the {} of Premise?\n",
    "    Answer:\n",
    "    \n",
    "    \"\"\".format(\n",
    "        example['premise'],\n",
    "        example['question'],\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'The patient underwent the risky medical procedure.',\n",
       " 'choice1': 'The procedure was costly.',\n",
       " 'choice2': 'Specialists recommended the procedure.',\n",
       " 'question': 'cause',\n",
       " 'idx': 360,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa[\"train\"][360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(example):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=generate_prompt_direct(example),\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "import time\n",
    "from torchmetrics.functional.text.rouge import rouge_score\n",
    "from cider.cider import Cider\n",
    "\n",
    "cider = Cider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(response):\n",
    "    ans = response.choices[0].text\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_direct(examples, sleep_time=1):\n",
    "    #labels = np.array(examples['label'])\n",
    "    premises = examples['premise']\n",
    "    questions = examples['question']\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"premise\"])):\n",
    "        if examples[\"label\"][i] == 0:\n",
    "            labels.append(examples[\"choice1\"][i])\n",
    "        else:\n",
    "            labels.append(examples[\"choice2\"][i])\n",
    "    bleu_scores = []\n",
    "    bleu_scores_1 = []\n",
    "    bleu_scores_2 = []\n",
    "    bleu_scores_3 = []\n",
    "    bleu_scores_4 = []\n",
    "    meteor_scores = []\n",
    "    anss = []\n",
    "    rouge_scores = [] \n",
    "    responses = []\n",
    "    for i in range(len(premises)):\n",
    "        example = { \n",
    "            'premise': premises[i],\n",
    "            'question': questions[i],\n",
    "        }\n",
    "        res = index(example)\n",
    "        ans = generate_prediction(res)\n",
    "        time.sleep(sleep_time)\n",
    "        responses.append(res)\n",
    "        anss.append(ans)\n",
    "        meteor_score = round(meteor([word_tokenize(ans)], word_tokenize(labels[i])))\n",
    "        meteor_scores.append(meteor_score)\n",
    "        bleu_score_4 = sentence_bleu(labels[i].split(), ans)#,weights=(1, 0, 0, 0))\n",
    "        bleu_score_3 = sentence_bleu(labels[i].split(), ans,weights=(0, 0, 1, 0))\n",
    "        bleu_score_2 = sentence_bleu(labels[i].split(), ans,weights=(0, 1, 0, 0))\n",
    "        bleu_score_1 = sentence_bleu(labels[i].split(), ans,weights=(1, 0, 0, 0))\n",
    "        bleu_score = np.exp(np.log(bleu_score_1) + 0.5*np.log(bleu_score_2) + 1/3*np.log(bleu_score_3)+0.25*np.log(bleu_score_4))\n",
    "        bleu_scores.append(bleu_score)\n",
    "        bleu_scores_1.append(bleu_score_1)\n",
    "        bleu_scores_2.append(bleu_score_2)\n",
    "        bleu_scores_3.append(bleu_score_3)\n",
    "        bleu_scores_4.append(bleu_score_4)\n",
    "        rouge_ = rouge_score(ans, labels[i])\n",
    "        rouge_scores.append(rouge_)\n",
    "    cider_score = cider.compute(anss, [[sen] for sen in labels])[0]\n",
    "    return bleu_scores,bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels, responses, anss, rouge_scores, meteor_scores, cider_score# rouge_score# meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/ccr/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/root/miniconda3/envs/ccr/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/root/miniconda3/envs/ccr/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_scores,bleu_scores_1,bleu_scores_2,bleu_scores_3,bleu_scores_4, labels,responses, anss, rouge_scores, meteor_scores,cider_score = test_direct(copa['validation'][:2], sleep_time=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores_average = np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0674562941586095"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores1_average = np.mean(bleu_scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39226258944483483"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores1_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39226258944483483\n",
      "0.233033719058363\n",
      "0.14286807132901955\n",
      "0.1481415083421549\n"
     ]
    }
   ],
   "source": [
    "bleu_scores1_average = np.mean(bleu_scores_1)\n",
    "bleu_scores2_average = np.mean(bleu_scores_2)\n",
    "bleu_scores3_average = np.mean(bleu_scores_3)\n",
    "bleu_scores4_average = np.mean(bleu_scores_4)\n",
    "print(bleu_scores1_average)\n",
    "print(bleu_scores2_average)\n",
    "print(bleu_scores3_average)\n",
    "print(bleu_scores4_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scores_average = np.mean(meteor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1507619"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_list = []\n",
    "for rouge_ in rouge_scores:\n",
    "    rouge_list.append(rouge_['rouge2_recall'])\n",
    "rouge_average = np.mean(rouge_list)\n",
    "rouge_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921265356324177"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_response(r, example, ans):\n",
    "    print(example)\n",
    "    #print(r.choices[0].text)\n",
    "    print(\"Answer :\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The man turned on the faucet.', 'choice1': 'The toilet filled with water.', 'choice2': 'Water flowed from the spout.', 'question': 'effect', 'idx': 0, 'label': 1}\n",
      "Answer : Water flowed from the faucet.\n",
      "{'premise': 'The girl found a bug in her cereal.', 'choice1': 'She poured milk in the bowl.', 'choice2': 'She lost her appetite.', 'question': 'effect', 'idx': 1, 'label': 1}\n",
      "Answer : She was disgusted and threw the cereal away.\n",
      "{'premise': 'The woman retired.', 'choice1': 'She received her pension.', 'choice2': 'She paid off her mortgage.', 'question': 'effect', 'idx': 2, 'label': 0}\n",
      "Answer : She stopped working and began collecting her pension.\n",
      "{'premise': 'I wanted to conserve energy.', 'choice1': 'I swept the floor in the unoccupied room.', 'choice2': 'I shut off the light in the unoccupied room.', 'question': 'effect', 'idx': 3, 'label': 1}\n",
      "Answer : I turned off the lights when I left the room.\n",
      "{'premise': 'The hamburger meat browned.', 'choice1': 'The cook froze it.', 'choice2': 'The cook grilled it.', 'question': 'cause', 'idx': 4, 'label': 1}\n",
      "Answer : It was placed on a hot grill.\n",
      "{'premise': \"I doubted the salesman's pitch.\", 'choice1': 'I turned his offer down.', 'choice2': 'He persuaded me to buy the product.', 'question': 'effect', 'idx': 5, 'label': 0}\n",
      "Answer : I asked for proof of the product's benefits.\n",
      "{'premise': 'I decided to stay home for the night.', 'choice1': 'The forecast called for storms.', 'choice2': 'My friends urged me to go out.', 'question': 'cause', 'idx': 6, 'label': 0}\n",
      "Answer : I was feeling tired and wanted to rest.\n",
      "{'premise': 'My eyes became red and puffy.', 'choice1': 'I was sobbing.', 'choice2': 'I was laughing.', 'question': 'cause', 'idx': 7, 'label': 0}\n",
      "Answer : I was crying.\n",
      "{'premise': 'The flame on the candle went out.', 'choice1': 'I blew on the wick.', 'choice2': 'I put a match to the wick.', 'question': 'cause', 'idx': 8, 'label': 0}\n",
      "Answer : The candle burned down to the wick.\n",
      "{'premise': 'The man drank heavily at the party.', 'choice1': 'He had a headache the next day.', 'choice2': 'He had a runny nose the next day.', 'question': 'effect', 'idx': 9, 'label': 0}\n",
      "Answer : He became very intoxicated.\n",
      "{'premise': 'The bowling ball knocked over the bowling pins.', 'choice1': 'The man rolled the bowling ball down the alley.', 'choice2': 'The man dropped the bowling ball on his foot.', 'question': 'cause', 'idx': 10, 'label': 0}\n",
      "Answer : The bowling ball was thrown down the lane.\n",
      "{'premise': \"The community learned of the man's death.\", 'choice1': 'His family buried him in the cemetery.', 'choice2': 'His obituary appeared in the newspaper.', 'question': 'cause', 'idx': 11, 'label': 1}\n",
      "Answer : He had passed away.\n",
      "{'premise': 'My computer crashed.', 'choice1': 'I installed new speakers.', 'choice2': 'I lost all my data.', 'question': 'effect', 'idx': 12, 'label': 1}\n",
      "Answer : I had to restart it.\n",
      "{'premise': 'The woman resigned from her job.', 'choice1': 'She aspired to hold an executive position in the firm.', 'choice2': 'She believed her superiors were acting unethically.', 'question': 'cause', 'idx': 13, 'label': 1}\n",
      "Answer : She was unhappy with her working conditions.\n",
      "{'premise': 'The player caught the ball.', 'choice1': 'Her teammate threw it to her.', 'choice2': 'Her opponent tried to intercept it.', 'question': 'cause', 'idx': 14, 'label': 0}\n",
      "Answer : The ball was thrown in his direction.\n",
      "{'premise': 'The judge pounded the gavel.', 'choice1': 'The courtroom broke into uproar.', 'choice2': 'The jury announced its verdict.', 'question': 'cause', 'idx': 15, 'label': 0}\n",
      "Answer : The judge wanted to bring an end to the court proceedings.\n",
      "{'premise': 'The woman banished the children from her property.', 'choice1': 'The children hit a ball into her yard.', 'choice2': 'The children trampled through her garden.', 'question': 'cause', 'idx': 16, 'label': 1}\n",
      "Answer : The children were creating a disturbance.\n",
      "{'premise': 'The kidnappers released the hostage.', 'choice1': 'They accepted ransom money.', 'choice2': 'They escaped from jail.', 'question': 'cause', 'idx': 17, 'label': 0}\n",
      "Answer : The kidnappers received a ransom payment.\n",
      "{'premise': \"The cook's eyes watered.\", 'choice1': 'He ran out of onions.', 'choice2': 'He cut an onion.', 'question': 'cause', 'idx': 18, 'label': 1}\n",
      "Answer : He was chopping onions.\n",
      "{'premise': 'The woman ran her finger under cold water.', 'choice1': 'She burned her finger on the toaster.', 'choice2': 'She put a diamond ring on her finger.', 'question': 'cause', 'idx': 19, 'label': 0}\n",
      "Answer : She burned her finger.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    show_example_response(responses[i], copa['validation'][i], anss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predict_file = \"./generations_gpt3.txt\"\n",
    "\n",
    "with open(output_predict_file, \"w\") as writer:\n",
    "    writer.write(\"index\\tprediction\\n\")\n",
    "    for index, item in enumerate(anss):\n",
    "        writer.write(f\"{index}\\t\\t{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2115e6518f0deb2150f8874acb24f1345316df487d5eca9760b49a6975aa76aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
